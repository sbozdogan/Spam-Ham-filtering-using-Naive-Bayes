{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir1 = 'D:\\\\Users\\\\sebozdogan\\\\Desktop\\\\Data Science\\\\Assignment 1\\\\Question 2 datasets\\\\enron1'\n",
    "rootdir2 = 'D:\\\\Users\\\\sebozdogan\\\\Desktop\\\\Data Science\\\\Assignment 1\\\\Question 2 datasets\\\\enron6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Users\\sebozdogan\\Desktop\\Data Science\\Assignment 1\\Question 2 datasets\\enron1 ['ham', 'spam'] 1\n",
      "D:\\Users\\sebozdogan\\Desktop\\Data Science\\Assignment 1\\Question 2 datasets\\enron1\\ham [] 3672\n",
      "D:\\Users\\sebozdogan\\Desktop\\Data Science\\Assignment 1\\Question 2 datasets\\enron1\\spam [] 1500\n",
      "D:\\Users\\sebozdogan\\Desktop\\Data Science\\Assignment 1\\Question 2 datasets\\enron6 ['ham', 'spam'] 1\n",
      "D:\\Users\\sebozdogan\\Desktop\\Data Science\\Assignment 1\\Question 2 datasets\\enron6\\ham [] 1500\n",
      "D:\\Users\\sebozdogan\\Desktop\\Data Science\\Assignment 1\\Question 2 datasets\\enron6\\spam [] 4500\n"
     ]
    }
   ],
   "source": [
    "#See directories and sub-directories\n",
    "for directories, subdirs, files in os.walk(rootdir1):\n",
    "    print(directories, subdirs, len(files))\n",
    "\n",
    "for directories, subdirs, files in os.walk(rootdir2):\n",
    "    print(directories, subdirs, len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Users\\sebozdogan\\Desktop\\Data Science\\Assignment 1\\Question 2 datasets\\enron1\\ham [] 3672\n",
      "D:\\Users\\sebozdogan\\Desktop\\Data Science\\Assignment 1\\Question 2 datasets\\enron1\\spam [] 1500\n",
      "D:\\Users\\sebozdogan\\Desktop\\Data Science\\Assignment 1\\Question 2 datasets\\enron6\\ham [] 1500\n",
      "D:\\Users\\sebozdogan\\Desktop\\Data Science\\Assignment 1\\Question 2 datasets\\enron6\\spam [] 4500\n"
     ]
    }
   ],
   "source": [
    "#Print ham and spam folders only to check if i am in the correct directory, then read\n",
    "for directories, subdirs, files in os.walk(rootdir1):\n",
    "    if (os.path.split(directories)[1]  == 'ham'): \n",
    "        print(directories, subdirs, len(files))\n",
    "    \n",
    "    if (os.path.split(directories)[1]  == 'spam'):\n",
    "        print(directories, subdirs, len(files))\n",
    "        \n",
    "for directories, subdirs, files in os.walk(rootdir2):\n",
    "    if (os.path.split(directories)[1]  == 'ham'): \n",
    "        print(directories, subdirs, len(files))\n",
    "    \n",
    "    if (os.path.split(directories)[1]  == 'spam'):\n",
    "        print(directories, subdirs, len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a function that returns true for each word\n",
    "def create_word_features(words):\n",
    "    result = dict( [ (word, True) for word in words] )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     D:\\Users\\sebozdogan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "({'Subject': True, ':': True, 'christmas': True, 'tree': True, 'farm': True, 'pictures': True}, 'ham')\n",
      "({'Subject': True, ':': True, 'dobmeos': True, 'with': True, 'hgh': True, 'my': True, 'energy': True, 'level': True, 'has': True, 'gone': True, 'up': True, '!': True, 'stukm': True, 'introducing': True, 'doctor': True, '-': True, 'formulated': True, 'human': True, 'growth': True, 'hormone': True, 'also': True, 'called': True, 'is': True, 'referred': True, 'to': True, 'in': True, 'medical': True, 'science': True, 'as': True, 'the': True, 'master': True, '.': True, 'it': True, 'very': True, 'plentiful': True, 'when': True, 'we': True, 'are': True, 'young': True, ',': True, 'but': True, 'near': True, 'age': True, 'of': True, 'twenty': True, 'one': True, 'our': True, 'bodies': True, 'begin': True, 'produce': True, 'less': True, 'by': True, 'time': True, 'forty': True, 'nearly': True, 'everyone': True, 'deficient': True, 'and': True, 'at': True, 'eighty': True, 'production': True, 'normally': True, 'diminished': True, 'least': True, '90': True, '95': True, '%': True, 'advantages': True, 'increased': True, 'muscle': True, 'strength': True, 'loss': True, 'body': True, 'fat': True, 'bone': True, 'density': True, 'lower': True, 'blood': True, 'pressure': True, 'quickens': True, 'wound': True, 'healing': True, 'reduces': True, 'cellulite': True, 'improved': True, 'vision': True, 'wrinkle': True, 'disappearance': True, 'skin': True, 'thickness': True, 'texture': True, 'levels': True, 'sleep': True, 'emotional': True, 'stability': True, 'memory': True, 'mental': True, 'alertness': True, 'sexual': True, 'potency': True, 'resistance': True, 'common': True, 'illness': True, 'strengthened': True, 'heart': True, 'controlled': True, 'cholesterol': True, 'mood': True, 'swings': True, 'new': True, 'hair': True, 'color': True, 'restore': True, 'read': True, 'more': True, 'this': True, 'website': True, 'unsubscribe': True}, 'spam')\n",
      "({'Subject': True, ':': True, 'key': True, 'dates': True, 'and': True, 'impact': True, 'of': True, 'upcoming': True, 'sap': True, 'implementation': True, 'over': True, 'the': True, 'next': True, 'few': True, 'weeks': True, ',': True, 'project': True, 'apollo': True, 'beyond': True, 'will': True, 'conduct': True, 'its': True, 'final': True, '\\x01': True, ')': True, 'this': True, 'approximately': True, '12': True, '000': True, 'new': True, 'users': True, 'plus': True, 'all': True, 'existing': True, 'system': True, '.': True, 'brings': True, 'a': True, 'dynamic': True, 'to': True, 'enron': True, 'enhancing': True, 'timely': True, 'flow': True, 'sharing': True, 'specific': True, 'human': True, 'resources': True, 'procurement': True, 'financial': True, 'information': True, 'across': True, 'business': True, 'units': True, 'continents': True, 'retire': True, 'multiple': True, 'disparate': True, 'systems': True, 'replace': True, 'them': True, 'with': True, 'common': True, 'integrated': True, 'encompassing': True, 'many': True, 'processes': True, 'including': True, 'payroll': True, 'timekeeping': True, 'benefits': True, 'management': True, 'numerous': True, 'employees': True, 'be': True, 'empowered': True, 'update': True, '/': True, 'or': True, 'view': True, 'their': True, 'personal': True, 'via': True, 'intranet': True, '-': True, 'based': True, 'ehronline': True, 'single': True, 'front': True, 'end': True, \"'\": True, 's': True, 'self': True, 'service': True, 'functionality': True, 'global': True, '(': True, 'gis': True, 'among': True, 'other': True, 'things': True, 'individuals': True, 'able': True, 'w': True, '4': True, 'addresses': True, 'banking': True, 'manage': True, 'individual': True, 'time': True, 'using': True, 'entry': True, 'tool': True, 'benefit': True, 'elections': True, 'on': True, 'line': True, 'paid': True, 'out': True, 'corporate': True, 'in': True, 'houston': True, 'excluding': True, 'azurix': True, 'communities': True, 'energy': True, 'services': True, 'investment': True, 'partners': True, 'north': True, 'america': True, 'renewable': True, 'corporation': True, 'gas': True, 'pipeline': True, 'group': True, 'finance': True, 'it': True, 'networks': True, 'products': True, 'e': True, '&': True, 'p': True, 'engineering': True, 'construction': True, 'company': True, 'only': True, 'international': True, 'regions': True, 'currently': True, 'supported': True, 'by': True, 'center': True, 'expertise': True, 'coe': True, 'london': True, 'people': True, 'impacted': True, 'gradually': True, 'june': True, 'current': True, 'may': True, 'notice': True, 'use': True, 'features': True, 'some': True, 'modules': True, 'was': True, 'developed': True, 'meet': True, 'requirements': True, 'implementing': True, 'as': True, 'part': True, '22': True, 'available': True, 'for': True, 'coding': True, 'must': True, 'used': True, 'timesheets': True, 'ids': True, '30': True, 'deadline': True, '!': True, 'period': True, 'beginning': True, '16': True, 'th': True, 'ending': True, 'entered': True, 'into': True, '3': True, '00': True, 'cst': True, 'expenses': True, 'invoices': True, 'july': True, '5': True, 'remaining': True, 'financials': True, 'are': True, 'more': True, 'visit': True, 'us': True, 'at': True, 'an': True, 'booth': True, 'building': True, 'lobby': True, 'wednesday': True, '7': True, 'thursday': True, '8': True, '10': True, 'm': True, 'till': True, '2': True, 'each': True, 'day': True, 'our': True, 'site': True, 'http': True, '\\\\': True, 'com': True, 'job': True, 'aids': True, 'useful': True, 'contact': True, 'manager': True, 'coordinating': True, 'within': True, 'your': True, 'unit': True, 'function': True, 'can': True, 'found': True, 'production': True, 'support': True, 'questions': True, 'telephone': True, '713': True, '345': True, 'mail': True, '@': True}, 'ham')\n",
      "({'Subject': True, ':': True, 'advs': True, 'greetings': True, ',': True, 'i': True, 'am': True, 'benedicta': True, 'lindiwe': True, 'hendricks': True, '(': True, 'mrs': True, ')': True, 'of': True, 'rsa': True, '.': True, 'writing': True, 'this': True, 'letter': True, 'to': True, 'you': True, 'with': True, 'the': True, 'hope': True, 'that': True, 'will': True, 'be': True, 'kind': True, 'enough': True, 'assist': True, 'my': True, 'family': True, 'if': True, 'means': True, 'communication': True, 'is': True, 'not': True, 'acceptable': True, 'please': True, 'accept': True, 'apologies': True, 'as': True, 'it': True, 'only': True, 'available': True, 'and': True, 'resourceful': True, 'for': True, 'me': True, 'right': True, 'now': True, 'children': True, 'are': True, 'in': True, 'need': True, 'your': True, 'assistance': True, 'we': True, 'sincerely': True, 'pray': True, 'able': True, 'attend': True, 'our': True, 'request': True, 'there': True, 'possibility': True, 'help': True, 'us': True, 'do': True, 'kindly': True, 'let': True, 'know': True, 'by': True, 'return': True, 'mail': True, 'so': True, 'can': True, 'tell': True, 'about': True, 'humble': True, 'thank': True, 'understanding': True, 'reply': True, 'email': True, 'address': True, ';': True, 'heno': True, '0': True, '@': True, 'katamail': True, 'com': True}, 'spam')\n"
     ]
    }
   ],
   "source": [
    "#Read ham and spam folders into seperate lists\n",
    "ham_list_enron1 = []\n",
    "spam_list_enron1 = []\n",
    "ham_list_enron6 = []\n",
    "spam_list_enron6 = []\n",
    "\n",
    "#read the files, and append them to the ham and spam list\n",
    "#encoding is needed to encounter unicode error\n",
    "#break the sentences into words using word_tokenize\n",
    "#use previous create_word_features function\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "    \n",
    "for directories, subdirs, files in os.walk(rootdir1):\n",
    "    if (os.path.split(directories)[1]  == 'ham'):\n",
    "        for filename in files:      \n",
    "            with open(os.path.join(directories, filename), encoding=\"latin-1\") as f:\n",
    "                data = f.read()\n",
    "                words = word_tokenize(data)\n",
    "                \n",
    "                #append ham at the end to tell the ML algorithm that type is ham\n",
    "                ham_list_enron1.append((create_word_features(words), \"ham\"))\n",
    "    \n",
    "    if (os.path.split(directories)[1]  == 'spam'):\n",
    "        for filename in files:\n",
    "            with open(os.path.join(directories, filename), encoding=\"latin-1\") as f:\n",
    "                data = f.read()\n",
    "                words = word_tokenize(data)\n",
    "                \n",
    "                #append spam at the end to tell the ML algorithm that type is spam\n",
    "                spam_list_enron1.append((create_word_features(words), \"spam\"))\n",
    "\n",
    "print(ham_list_enron1[0])\n",
    "print(spam_list_enron1[0])\n",
    "\n",
    "for directories, subdirs, files in os.walk(rootdir2):\n",
    "    if (os.path.split(directories)[1]  == 'ham'):\n",
    "        for filename in files:      \n",
    "            with open(os.path.join(directories, filename), encoding=\"latin-1\") as f:\n",
    "                data = f.read()\n",
    "                words = word_tokenize(data)\n",
    "                \n",
    "                #append ham at the end to tell the ML algorithm that type is ham\n",
    "                ham_list_enron6.append((create_word_features(words), \"ham\"))\n",
    "    \n",
    "    if (os.path.split(directories)[1]  == 'spam'):\n",
    "        for filename in files:\n",
    "            with open(os.path.join(directories, filename), encoding=\"latin-1\") as f:\n",
    "                data = f.read()\n",
    "                words = word_tokenize(data)\n",
    "                \n",
    "                #append spam at the end to tell the ML algorithm that type is spam\n",
    "                spam_list_enron6.append((create_word_features(words), \"spam\"))\n",
    "\n",
    "print(ham_list_enron6[0])\n",
    "print(spam_list_enron6[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5172\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "#Combine and shuffle ham and spam mails before splitting the data into train and test\n",
    "combined_list1 = ham_list_enron1 + spam_list_enron1\n",
    "print(len(combined_list1))\n",
    "\n",
    "random.shuffle(combined_list1)\n",
    "\n",
    "combined_list2 = ham_list_enron6 + spam_list_enron6\n",
    "print(len(combined_list2))\n",
    "\n",
    "random.shuffle(combined_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5172\n",
      "3620\n",
      "1552\n",
      "6000\n",
      "4200\n",
      "1800\n"
     ]
    }
   ],
   "source": [
    "#Split data into training and testing \n",
    "train1 = int(len(combined_list1) * 0.7)\n",
    "\n",
    "print(len(combined_list1))\n",
    "\n",
    "training_set1 = combined_list1[:train1]\n",
    "\n",
    "testing_set1 =  combined_list1[train1:]\n",
    "\n",
    "print (len(training_set1))\n",
    "print (len(testing_set1))\n",
    "\n",
    "train2 = int(len(combined_list2) * 0.7)\n",
    "\n",
    "print(len(combined_list2))\n",
    "\n",
    "training_set2 = combined_list2[:train2]\n",
    "\n",
    "testing_set2 =  combined_list2[train2:]\n",
    "\n",
    "print (len(training_set2))\n",
    "print (len(testing_set2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the Naive Bayes Classifier into training_Set\n",
    "classifier1 = NaiveBayesClassifier.train(training_set1)\n",
    "classifier2 = NaiveBayesClassifier.train(training_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for enron 1 classifier is:  94.90979381443299\n",
      "Accuracy for enron 6 classifier is:  96.72222222222221\n"
     ]
    }
   ],
   "source": [
    "#Test the model in testing_set and calculate accuracy\n",
    "accuracy1 = nltk.classify.util.accuracy(classifier1, testing_set1)\n",
    "print(\"Accuracy for enron 1 classifier is: \", accuracy1 * 100)\n",
    "\n",
    "accuracy2 = nltk.classify.util.accuracy(classifier2, testing_set2)\n",
    "print(\"Accuracy for enron 6 classifier is: \", accuracy2 * 100)\n",
    "\n",
    "#Because accuracy rates are calculated from both ham and spam predictions, \n",
    "#lets look at confusion matrices to compare their power on detecting especially spam e-mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'spam']\n",
      "Confusion Matrix for enron 1 classifier\n",
      "      |         s |\n",
      "     |    h    p |\n",
      "     |    a    a |\n",
      "     |    m    m |\n",
      "-----+-----------+\n",
      " ham |<1031>  71 |\n",
      "spam |    8 <442>|\n",
      "-----+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "['spam', 'ham']\n",
      "Confusion Matrix for enron 6 classifier\n",
      "      |         s |\n",
      "     |    h    p |\n",
      "     |    a    a |\n",
      "     |    m    m |\n",
      "-----+-----------+\n",
      " ham | <445>   3 |\n",
      "spam |   56<1296>|\n",
      "-----+-----------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Report confusion matrices\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "\n",
    "print(classifier1.labels())\n",
    "\n",
    "testing_result1 = []\n",
    "gold_result1 = []\n",
    "\n",
    "for i in range(len(testing_set1)):\n",
    "    testing_result1.append(classifier1.classify(testing_set1[i][0]))\n",
    "    gold_result1.append(testing_set1[i][1])\n",
    "\n",
    "print('Confusion Matrix for enron 1 classifier\\n' , nltk.ConfusionMatrix(gold_result1, testing_result1))\n",
    "\n",
    "print(classifier2.labels())\n",
    "\n",
    "testing_result2 = []\n",
    "gold_result2 = []\n",
    "\n",
    "for i in range(len(testing_set2)):\n",
    "    testing_result2.append(classifier2.classify(testing_set2[i][0]))\n",
    "    gold_result2.append(testing_set2[i][1])\n",
    "\n",
    "print('Confusion Matrix for enron 6 classifier\\n' , nltk.ConfusionMatrix(gold_result2, testing_result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                     hou = True              ham : spam   =    164.7 : 1.0\n",
      "                    2004 = True             spam : ham    =    137.8 : 1.0\n",
      "            prescription = True             spam : ham    =    106.8 : 1.0\n",
      "                     ect = True              ham : spam   =    102.4 : 1.0\n",
      "                    pain = True             spam : ham    =     97.0 : 1.0\n",
      "                  stocks = True             spam : ham    =     79.1 : 1.0\n",
      "                     sex = True             spam : ham    =     75.8 : 1.0\n",
      "                    spam = True             spam : ham    =     74.2 : 1.0\n",
      "                  differ = True             spam : ham    =     70.9 : 1.0\n",
      "                      ex = True             spam : ham    =     66.0 : 1.0\n",
      "                  health = True             spam : ham    =     62.8 : 1.0\n",
      "                 generic = True             spam : ham    =     62.8 : 1.0\n",
      "             subscribers = True             spam : ham    =     61.2 : 1.0\n",
      "                creative = True             spam : ham    =     59.5 : 1.0\n",
      "                  weight = True             spam : ham    =     57.9 : 1.0\n",
      "                      cc = True              ham : spam   =     56.4 : 1.0\n",
      "                     ibm = True             spam : ham    =     56.3 : 1.0\n",
      "                  farmer = True              ham : spam   =     53.3 : 1.0\n",
      "                    2001 = True              ham : spam   =     53.1 : 1.0\n",
      "                conflict = True             spam : ham    =     53.0 : 1.0\n",
      "Most Informative Features\n",
      "                      tw = True              ham : spam   =    193.5 : 1.0\n",
      "                pipeline = True              ham : spam   =    174.0 : 1.0\n",
      "                     713 = True              ham : spam   =    160.2 : 1.0\n",
      "                lorraine = True              ham : spam   =    144.5 : 1.0\n",
      "                   hyatt = True              ham : spam   =    144.5 : 1.0\n",
      "                 lateral = True              ham : spam   =    144.5 : 1.0\n",
      "                kimberly = True              ham : spam   =    132.6 : 1.0\n",
      "                    paso = True              ham : spam   =    119.0 : 1.0\n",
      "                       \u0001 = True              ham : spam   =    108.7 : 1.0\n",
      "                     ets = True              ham : spam   =    103.5 : 1.0\n",
      "                  tariff = True              ham : spam   =     84.7 : 1.0\n",
      "                   kevin = True              ham : spam   =     84.3 : 1.0\n",
      "                 permian = True              ham : spam   =     82.7 : 1.0\n",
      "                 houston = True              ham : spam   =     81.8 : 1.0\n",
      "                     853 = True              ham : spam   =     80.7 : 1.0\n",
      "                      pg = True              ham : spam   =     79.0 : 1.0\n",
      "                     kay = True              ham : spam   =     76.8 : 1.0\n",
      "               pipelines = True              ham : spam   =     72.8 : 1.0\n",
      "                lindberg = True              ham : spam   =     67.6 : 1.0\n",
      "                     dth = True              ham : spam   =     66.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#See the most important features(words) of classification\n",
    "classifier1.show_most_informative_features(20)\n",
    "classifier2.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message is : spam\n",
      "Message is : spam\n"
     ]
    }
   ],
   "source": [
    "#Classify an example message\n",
    "msg = '''we analyse the Enron email dataset 2001, half a thousand files, deciding if a mail is spam or not'''\n",
    "words1 = word_tokenize(msg)\n",
    "features = create_word_features(words1)\n",
    "print(\"Message is :\" ,classifier1.classify(features))\n",
    "print(\"Message is :\" ,classifier2.classify(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
